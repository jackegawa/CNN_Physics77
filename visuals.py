"""
Visuals Library for Deep Learning Benchmarking
==============================================

This library provides a suite of plotting functions to analyze and compare 
training logs generated by Custom CNN frameworks and PyTorch baselines.

It is designed to be imported as a module in Jupyter Notebooks.

Usage Examples
--------------

1. **Import the library**:
    >>> import visuals
    >>> import glob

2. **Compare Loss & Accuracy (Custom vs PyTorch)**:
    >>> custom_log = "./logs/benchmark_custom.json"
    >>> torch_log = "./logs/benchmark_torch.json"
    >>> visuals.plot_loss_comparison(custom=custom_log, torch=torch_log)
    >>> visuals.plot_accuracy_comparison(custom=custom_log, torch=torch_log)

3. **Analyze Training Stability (Gradient Norms)**:
    >>> visuals.plot_gradient_norm(custom_log)

4. **Visualize Confusion Matrix**:
    >>> visuals.plot_confusion_matrix(custom_log)

5. **Architecture Search Analysis (Efficiency Frontier)**:
    # Plot all JSON logs in the directory to compare speed vs accuracy
    >>> all_logs = glob.glob("./logs/*.json")
    >>> visuals.plot_efficiency_frontier(all_logs)

Dependencies
------------
- matplotlib
- numpy
- seaborn (optional, for prettier heatmaps)
"""

import json
import matplotlib.pyplot as plt
import os
import numpy as np
import re
from typing import List, Optional, Union

# Attempt to import seaborn for better heatmaps; degrade gracefully if not found.
try:
    import seaborn as sns
    HAS_SNS = True
except ImportError:
    HAS_SNS = False

# --- Global Style Settings ---
# Set a professional plotting style
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams.update({
    'font.family': 'sans-serif',
    'font.size': 11,
    'axes.titlesize': 14,
    'lines.linewidth': 2,
    'figure.dpi': 120,
    'figure.figsize': (10, 6)
})

# =========================================================
#                 Private Helper Functions
# =========================================================

def _load_log(path: str) -> Optional[dict]:
    """
    Internal helper to safely load a JSON log file.
    
    Args:
        path (str): The file path to the .json log.
        
    Returns:
        dict: The loaded data dict, or None if loading fails.
    """
    if not os.path.exists(path):
        print(f"[Error] File not found: {path}")
        return None
    try:
        with open(path, 'r') as f:
            data = json.load(f)
            # Inject filename into data for easier labeling later
            data['_filename'] = os.path.basename(path) 
            return data
    except Exception as e:
        print(f"[Error] Failed to load json: {e}")
        return None

def _get_label(log_data: dict, custom_name: str = None) -> str:
    """
    Internal helper to generate a descriptive legend label.
    
    It attempts to extract:
    1. Framework name (Torch vs Custom)
    2. Optimizer type
    3. Architecture details (Kernel, Channels, Hidden Dim) from filename regex.
    """
    if custom_name:
        return custom_name
    
    fname = log_data.get('_filename', '')
    cfg = log_data.get('config', {})
    fw = log_data["config"]["model_name"]
    opt_conf = cfg.get('optimizer', {})
    
    # --- Optimizer Name Extraction ---
    # Robustly handle different log formats (dict vs string)
    if isinstance(opt_conf, dict):
        # 'type' is used by PyTorch logs, 'optimizer' by Custom logs (optim.py)
        opt = opt_conf.get('type') or opt_conf.get('optimizer') or 'UnknownOpt'
    else:
        # Fallback for older logs where optimizer might be a raw string
        opt_str = str(opt_conf)
        if "Adam" in opt_str: opt = "Adam"
        elif "SGD" in opt_str: opt = "SGD"
        else: opt = "Unknown"
    
    # --- Framework Detection ---
    # Heuristic: check if filename starts with "Torch"
    if fw == "TorchCNN":
        prefix = "PyTorch"
    elif fw == "CNN":
        prefix = "Custom"
    else:
        prefix = None

    # --- Architecture Parsing ---
    # Regex to find patterns like "_k3_c8_h128" in the filename
    match = re.search(r"_k(\d+)_c(\d+)_h(\d+)", fname)
    if match:
        k, c, h = match.groups()
        return f"{prefix} {opt} (K={k}, C={c}, H={h})"
    
    return f"{prefix} {opt}"

def _smooth(data: list, window: int = 50) -> np.ndarray:
    """
    Applies a simple moving average to smooth out noisy curves (like loss).
    """
    if not data or len(data) <= window:
        return np.array(data)
    return np.convolve(data, np.ones(window)/window, mode='valid')

# =========================================================
#                 Public Plotting API
# =========================================================

def plot_loss_comparison(
    custom: str, 
    torch: str, 
    custom_label: str = "Custom Model", 
    torch_label: str = "PyTorch Baseline"
):
    """
    Plots a comparison of TRAINING LOSS per epoch between Custom and PyTorch models.

    Args:
        custom (str): Path to the Custom model's JSON log.
        torch (str): Path to the PyTorch model's JSON log.
        custom_label (str): Legend label for Custom model.
        torch_label (str): Legend label for PyTorch model.
    """
    paths = [(custom, custom_label), (torch, torch_label)]
    
    plt.figure()
    for path, label in paths:
        if not path: continue
        data = _load_log(path)
        if not data: continue
        
        stats = data['training_results']['per_epoch_stats']
        epochs = [s['epoch'] for s in stats]
        losses = [s['loss'] for s in stats]
        
        # Use dynamic label if user didn't provide a specific one, or append to it
        final_label = _get_label(data) if label is None else label
        
        plt.plot(epochs, losses, marker='o', label=final_label)
    
    plt.title("Training Loss Comparison (Lower is Better)")
    plt.xlabel("Epoch")
    plt.ylabel("Average Cross-Entropy Loss")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

def plot_accuracy_comparison(
    custom: str, 
    torch: str, 
    custom_label: str = "Custom Model", 
    torch_label: str = "PyTorch Baseline"
):
    """
    Plots a comparison of TEST ACCURACY per epoch between Custom and PyTorch models.

    Args:
        custom (str): Path to the Custom model's JSON log.
        torch (str): Path to the PyTorch model's JSON log.
    """
    paths = [(custom, custom_label), (torch, torch_label)]
    
    plt.figure()
    for path, label in paths:
        if not path: continue
        data = _load_log(path)
        if not data: continue
        
        stats = data['training_results']['per_epoch_stats']
        epochs = [s['epoch'] for s in stats]
        
        # Normalize accuracy to percentage (0-100) if it's in 0-1 range
        accs = [s['accuracy'] for s in stats]
        if accs and max(accs) <= 1.0:
            accs = [a * 100 for a in accs]
            
        final_label = _get_label(data) if label is None else label
        plt.plot(epochs, accs, marker='s', label=final_label)
    
    plt.title("Test Accuracy Comparison (Higher is Better)")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()

def plot_detailed_loss(path: str, window: int = 50, title: str = None):
    """
    Plots the detailed STEP-LEVEL loss. 
    Useful for diagnosing convergence stability within an epoch.

    Args:
        path (str): Path to the JSON log.
        window (int): Smoothing window size.
        title (str, optional): Custom title.
    """
    data = _load_log(path)
    if not data: return

    raw_loss = data['training_results'].get('detailed_loss', [])
    if not raw_loss:
        print("[Warn] No detailed loss data found.")
        return

    smooth_loss = _smooth(raw_loss, window)

    plt.figure(figsize=(12, 4))
    plt.plot(raw_loss, color='lightgray', alpha=0.5, label='Raw Step Loss')
    plt.plot(smooth_loss, linewidth=1.5, color='#1f77b4', label=f'Smoothed (MA={window})')
    
    final_title = title if title else f"Detailed Training Dynamics: {_get_label(data)}"
    plt.title(final_title)
    plt.xlabel("Step (Batch)")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()

def plot_gradient_norm(path: str, window: int = 50):
    """
    Plots the L2 NORM of gradients over time.
    Critical for debugging Vanishing or Exploding gradients.

    Args:
        path (str): Path to the JSON log.
    """
    data = _load_log(path)
    if not data: return

    grad_norms = data['training_results'].get('detailed_grad_norm', [])
    if not grad_norms:
        print("[Warn] No gradient norm data found.")
        return
    
    smooth_grad = _smooth(grad_norms, window)

    plt.figure(figsize=(12, 4))
    plt.plot(grad_norms, color='lightgreen', alpha=0.4, label='Raw Norm')
    plt.plot(smooth_grad, color='green', linewidth=1.5, label=f'Smoothed (MA={window})')
    
    plt.title(f"Gradient Stability Check: {_get_label(data)}")
    plt.xlabel("Step")
    plt.ylabel("L2 Norm of Gradients")
    plt.legend()
    plt.show()

def plot_confusion_matrix(path: str):
    """
    Visualizes the Confusion Matrix for the final epoch.
    Shows which digits are being confused with which (e.g., 4 vs 9).
    """
    data = _load_log(path)
    if not data: return

    eval_data = data['training_results'].get('final_evaluation', {})
    if not eval_data:
        print("[Warn] No confusion matrix data found.")
        return

    y_true = eval_data['y_true']
    y_pred = eval_data['y_pred']
    
    # Calculate 10x10 Matrix
    cm = np.zeros((10, 10), dtype=int)
    for t, p in zip(y_true, y_pred):
        cm[t][p] += 1

    plt.figure(figsize=(8, 7))
    if HAS_SNS:
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    else:
        plt.imshow(cm, cmap='Blues')
        plt.colorbar()
        for i in range(10):
            for j in range(10):
                plt.text(j, i, cm[i, j], ha="center", va="center", color="black")

    plt.title(f"Confusion Matrix: {_get_label(data)}")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.grid(False)
    plt.show()

def plot_efficiency_frontier(log_paths: List[str]):
    """
    Plots the 'Efficiency Frontier' (Pareto Frontier) bubble chart.
    
    Axes:
    - X: Number of Parameters (Log scale) -> Model Size
    - Y: Test Accuracy -> Model Performance
    - Color: Training Time -> Computational Cost
    
    Args:
        log_paths (List[str]): List of file paths to multiple JSON logs.
    """
    params, accs, times, labels = [], [], [], []

    for path in log_paths:
        data = _load_log(path)
        if not data: continue
        
        stats = data['training_results']['per_epoch_stats']
        cfg = data.get('config', {})
        if not stats: continue
        
        # Extract Metrics
        p_count = cfg.get('parameter_count', 0)
        final_acc = stats[-1]['accuracy'] * 100
        total_time = sum(s['time'] for s in stats)
        
        if p_count > 0:
            params.append(p_count)
            accs.append(final_acc)
            times.append(total_time)
            labels.append(_get_label(data))

    if not params:
        print("[Error] No valid data extracted from logs.")
        return

    plt.figure(figsize=(10, 6))
    # Scatter plot: x=params, y=accuracy, c=time, s=size
    scatter = plt.scatter(params, accs, c=times, s=150, cmap='viridis_r', edgecolors='black', alpha=0.8)
    
    cbar = plt.colorbar(scatter)
    cbar.set_label('Total Training Time (s)')
    
    plt.xscale('log')
    plt.xlabel("Number of Parameters (Log Scale)")
    plt.ylabel("Test Accuracy (%)")
    plt.title("Efficiency Frontier (Upper-Left is Better)")
    
    # Annotate points
    for i, label in enumerate(labels):
        plt.annotate(label, (params[i], accs[i]), xytext=(5, 5), textcoords='offset points', fontsize=9)
        
    plt.grid(True, which="both", ls="--", alpha=0.5)
    plt.show()

def plot_system_benchmark(custom_path: str, torch_path: str):
    """
    Plots a system benchmark comparison between Custom and PyTorch models.
    Args:
        custom_path (str): Path to the Custom model's JSON log.
        torch_path (str): Path to the PyTorch model's JSON log.
    """
    # 1. Load data
    with open(custom_path, 'r') as f: c_data = json.load(f)
    with open(torch_path, 'r') as f: t_data = json.load(f)
    
    c_stats = c_data['training_results']['per_epoch_stats']
    t_stats = t_data['training_results']['per_epoch_stats']
    
    # 2. Compute averages
    epochs = len(c_stats)
    c_time = np.mean([s['time'] for s in c_stats])
    t_time = np.mean([s['time'] for s in t_stats])
    
    c_mem = np.max([s['memory_peak'] for s in c_stats])
    t_mem = np.max([s['memory_peak'] for s in t_stats])
    
    # 3. Plotting
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # Bar chart parameters
    labels = ['Custom Framework', 'PyTorch']
    colors = ['#4c72b0', '#55a868']
    width = 0.5
    
    # Subplot 1: Training Time
    times = [c_time, t_time]
    bars1 = ax1.bar(labels, times, width, color=colors, alpha=0.8)
    ax1.set_ylabel('Avg Time per Epoch (seconds)')
    ax1.set_title(f'Training Speed Comparison (Lower is Better)')
    ax1.grid(axis='y', linestyle='--', alpha=0.5)
    
    # Annotate values
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f} s', ha='center', va='bottom', fontsize=12, fontweight='bold')

    # Subplot 2: Memory Usage
    mems = [c_mem, t_mem]
    bars2 = ax2.bar(labels, mems, width, color=colors, alpha=0.8)
    ax2.set_ylabel('Peak Memory Usage (MB)')
    ax2.set_title(f'Memory Efficiency Comparison (Lower is Better)')
    ax2.grid(axis='y', linestyle='--', alpha=0.5)
    
    # Annotate values
    for bar in bars2:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f} MB', ha='center', va='bottom', fontsize=12, fontweight='bold')
    
    # Calculate multiples difference and set as title
    speedup = c_time / t_time if t_time > 0 else 0
    mem_diff = c_mem / t_mem if t_mem > 0 else 0
    plt.suptitle(f"Benchmark Summary: PyTorch is {speedup:.1f}x Faster | Custom uses {mem_diff:.1f}x Memory", 
                fontsize=14, y=1.05)
    
    plt.tight_layout()
    plt.show()