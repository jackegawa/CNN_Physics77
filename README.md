# CNN_Physics77: NumPy-based Deep Learning Framework

This repository implements a Convolutional Neural Network (CNN) from scratch using **NumPy**, featuring a custom automatic differentiation engine (Autograd).

Designed as a computational physics project, this framework builds the deep learning stack from **first principles**. It includes a rigorous benchmarking suite that compares the custom implementation against a **PyTorch** baseline to validate mathematical correctness, convergence behavior, and memory efficiency.

## ðŸš€ Key Features

### 1. Core Implementations
+ **Custom Autograd Engine**: A dynamic computation graph with a `Tensor` class supporting gradient accumulation and automatic backpropagation (DAG).
+ **Vectorized Convolutions**: Implements `im2col` (**image-to-column**) and `col2im` algorithms to transform convolutions into efficient Matrix Multiplications (GEMM), leveraging NumPy's optimized BLAS routines.
+ **Modular Architecture**: Mimics PyTorch's API design with `Conv2D`, `Linear`, `ReLU`, and `Softmax` layers.
+ **Optimizers**: Custom implementations of **SGD** and **Adam** (with moment tracking and bias correction).

### 2. Scientific Benchmarking
An "Apples-to-Apples" comparison suite ensuring matched architecture, matched initialization scheme ($\mathcal{N}(0, 0.01)$), identical data, and controlled seeding for reproducibility to measure pure framework overhead:
+ **Efficiency Frontier**: Analyzing the trade-off between parameter count, accuracy, and training time.
+ **Gradient Stability**: Monitoring $||\nabla \theta||_2$ to ensure numerical stability in the custom backward pass.

## ðŸ“‚ Project Structure

```bash
CNN_Physics77/
â”œâ”€â”€ analysis/               # Visualization tools
â”‚   â””â”€â”€ visuals.py          # Plotting utilities (Loss, Accuracy, Efficiency Frontier)
â”œâ”€â”€ benchmarks/             # Benchmarking scripts
â”‚   â”œâ”€â”€ CNN_torch.py        # PyTorch Baseline (structurally aligned to custom model)
â”‚   â”œâ”€â”€ train.py            # Custom Framework Training Loop
â”‚   â””â”€â”€ helper.py           # Experiment orchestration & logging
â”œâ”€â”€ core/                   # The Custom Framework Library
â”‚   â”œâ”€â”€ tensor.py           # Autograd & Tensor class
â”‚   â”œâ”€â”€ operations.py       # Math Ops (im2col, Conv, ReLU, Softmax)
â”‚   â”œâ”€â”€ layers.py           # Layer definitions (Conv2D, Linear)
â”‚   â”œâ”€â”€ model.py            # CNN Architecture definition
â”‚   â”œâ”€â”€ optim.py            # Optimizers (SGD, Adam)
â”‚   â””â”€â”€ data.py             # MNIST Data Loader
â”œâ”€â”€ logs/                   # Training logs (JSON/TXT)
â”œâ”€â”€ fig/                    # Generated plots
â”œâ”€â”€ main.py                 # Entry point for running experiments
â””â”€â”€ README.md
```

## ðŸ“¦ Installation

1. **Environment Setup:**
    ```bash
    conda env create -f environment.yml
    conda activate cnn_physics77
    ```

2. **Clone the Repository:**
    ```bash
    git clone [https://github.com/jackegawa/CNN_Physics77](https://github.com/jackegawa/CNN_Physics77)
    cd CNN_Physics77
    ```

## ðŸ’» Usage

The project is controlled via `main.py`, offering different modes for training and analysis.

### 1. Basic Benchmark (Custom vs PyTorch)
Runs a single comparison to verify that the Custom Model converges similarly to the PyTorch baseline.
```bash
python main.py --mode basic --epochs 5 --optimizer Adam --lr 0.001
```

### 2. Optimizer Grid Search
Runs a 4-way comparison (Custom-SGD, Custom-Adam, Torch-SGD, Torch-Adam) to analyze optimizer implementation correctness.
```bash
python main.py --mode grid --epochs 5 --lr 0.001
```

### 3. Diagnostics Mode
Generates detailed step-level analysis (Gradient Norms, Step Loss) to debug vanishing/exploding gradients.
```bash
python main.py --mode basic --diagnostics --epochs 5 --optimizer SGD --lr 0.001
```

### CLI Arguments

| Argument       | Default | Description                                                 |
|----------------|---------|-------------------------------------------------------------|
| `--mode`       | `basic` | Run mode: `basic` (1v1 comparison) or `grid` (4-way search) |
| `--optimizer`  | `Adam`  | Optimizer choice: `SGD` or `Adam`                           |
| `--epochs`     | `5`     | Number of training epochs                                   |
| `--batch_size` | `64`    | Batch size                                                  |
| `--lr`         | `0.001` | Learning rate                                               |
| `--seed`       | `67`    | Random seed for reproducibility                             |
| `--diagnostics`| `False` | Enable detailed diagnostics output                          |

## ðŸ“Š Visualizations

The `analysis/visuals.py` module automatically generates plots in the `fig/` directory. Key metrics include:

1. **Efficiency Frontier**: A bubble chart comparing **Model Size vs. Accuracy vs. Training Time**.

2. **Gradient Norm Stability**: Line plots of gradient norms over training steps to validate that the custom backward pass is mathematically stable.

3. **Loss & Accuracy Curves**: Standard training curves to confirm that both frameworks follow the same optimization trajectory.

## ðŸ§  Implementation Details

### The `im2col` Operation
Instead of using slow nested loops for convolution, this framework uses `im2col` to flatten input patches into a matrix. This converts the convolution operation into a single large Matrix Multiplication (GEMM):

$$
\text{Output} = \text{Conv2D}(\text{Input}, \text{Filters}) \implies \text{Output\_matrix} = \text{Input\_matrix} \times \text{Filter\_matrix}
$$

### Autograd Engine
The `Tensor` class maintains a list of `parents` and an `op` (operation). During the forward pass, the graph is built dynamically. During `backward()`, gradients flow using the chain rule:

```python
# Simplified logic from core/tensor.py
def backward(self, grad):
    self.grad += grad
    if self.op:
        parent_grads = self.op.backward(self, grad)
        for parent, g in zip(self.parents, parent_grads):
            parent.backward(g)
```

> **Note on Performance:** As this framework is implemented purely in Python/NumPy for educational purposes, it is expected to be slower than PyTorch (which relies on C++/CUDA backends), especially for larger batch sizes.