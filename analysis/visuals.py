"""
Visuals Library for Deep Learning Benchmarking
==============================================

This library provides a suite of plotting functions to analyze and compare 
training logs generated by Custom CNN frameworks and PyTorch baselines.

It automatically saves all generated plots to a './fig/' directory using 
the plot title as the filename.

Dependencies
------------
- matplotlib
- numpy
- seaborn (optional, for prettier heatmaps)
"""

import json
import matplotlib.pyplot as plt
import os
import numpy as np
import re
from typing import List, Optional, Union, Tuple

# Attempt to import seaborn for better heatmaps; degrade gracefully if not found.
try:
    import seaborn as sns
    HAS_SNS = True
except ImportError:
    HAS_SNS = False

# --- Global Style Settings ---
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams.update({
    'font.family': 'sans-serif',
    'font.size': 11,
    'axes.titlesize': 14,
    'lines.linewidth': 2,
    'figure.dpi': 120,
    'figure.figsize': (10, 6)
})

# =========================================================
#                 Private Helper Functions
# =========================================================

def _save_plot(title: str):
    """
    Internal helper to save the current figure to ./fig/ folder.
    Filename is derived from the title (sanitized).
    """
    if not title:
        title = "untitled_plot"
        
    save_dir = "./fig"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
        
    # Sanitize title to create a valid filename
    # 1. Remove non-alphanumeric chars (except spaces and hyphens)
    clean_title = re.sub(r'[^\w\s-]', '', title)
    # 2. Replace spaces/hyphens with underscores
    clean_title = re.sub(r'[-\s]+', '_', clean_title).strip().lower()
    
    filename = f"{clean_title}.png"
    filepath = os.path.join(save_dir, filename)
    
    try:
        plt.savefig(filepath, bbox_inches='tight', dpi=300)
        print(f"[SAVE] Plot saved to: {filepath}")
    except Exception as e:
        print(f"[WARN] Failed to save plot '{filename}': {e}")

def _load_log(path: str) -> Optional[dict]:
    """
    Internal helper to safely load a JSON log file.
    """
    if not os.path.exists(path):
        print(f"[Error] File not found: {path}")
        return None
    try:
        with open(path, 'r') as f:
            data = json.load(f)
            data['_filename'] = os.path.basename(path) 
            return data
    except Exception as e:
        print(f"[Error] Failed to load json: {e}")
        return None

def _get_label(log_data: dict, custom_name: str = None) -> str:
    """
    Internal helper to generate a descriptive legend label.
    """
    if custom_name:
        return custom_name
    
    fname = log_data.get('_filename', '')
    cfg = log_data.get('config', {})
    fw = log_data["config"]["model_name"]
    opt_conf = cfg.get('optimizer', {})
    
    if isinstance(opt_conf, dict):
        opt = opt_conf.get('type') or opt_conf.get('optimizer') or 'UnknownOpt'
    else:
        opt_str = str(opt_conf)
        if "Adam" in opt_str: opt = "Adam"
        elif "SGD" in opt_str: opt = "SGD"
        else: opt = "Unknown"
    
    if fw == "TorchCNN":
        prefix = "PyTorch"
    elif fw == "CNN":
        prefix = "Custom"
    else:
        prefix = None

    match = re.search(r"_k(\d+)_c(\d+)_h(\d+)", fname)
    if match:
        k, c, h = match.groups()
        return f"{prefix} (K={k}, C={c}, H={h})", f"{opt}"
    
    return f"{prefix}", f"{opt}"

def _smooth(data: list, window: int = 50) -> np.ndarray:
    if not data or len(data) <= window:
        return np.array(data)
    return np.convolve(data, np.ones(window)/window, mode='valid')

# =========================================================
#                 Public Plotting API
# =========================================================

def plot_loss_comparison(
    custom: str, 
    torch: str, 
    custom_label: str = "Custom", 
    torch_label: str = "PyTorch"
):
    """Plots a comparison of TRAINING LOSS per epoch."""
    paths = [(custom, custom_label), (torch, torch_label)]
    
    plt.figure()
    for path, label in paths:
        if not path: continue
        data = _load_log(path)
        if not data: continue
        
        stats = data['training_results']['per_epoch_stats']
        epochs = [s['epoch'] for s in stats]
        losses = [s['loss'] for s in stats]
        
        if label is None:
            final_label, _ = _get_label(data)
        else:
            final_label = label
        
        plt.plot(epochs, losses, marker='o', label=final_label)
    
    title = "Training Loss Comparison"
    plt.title(title)
    plt.xlabel("Epoch")
    plt.ylabel("Average Cross-Entropy Loss")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    _save_plot(title)
    plt.show()

def plot_accuracy_comparison(
    custom: str, 
    torch: str, 
    custom_label: str = "Custom", 
    torch_label: str = "PyTorch"
):
    """Plots a comparison of TEST ACCURACY per epoch."""
    paths = [(custom, custom_label), (torch, torch_label)]
    
    plt.figure()
    for path, label in paths:
        if not path: continue
        data = _load_log(path)
        if not data: continue
        
        stats = data['training_results']['per_epoch_stats']
        epochs = [s['epoch'] for s in stats]
        
        accs = [s['accuracy'] for s in stats]
        if accs and max(accs) <= 1.0:
            accs = [a * 100 for a in accs]
            
        if label is None:
            final_label, _ = _get_label(data)
        else:
            final_label = label
        plt.plot(epochs, accs, marker='s', label=final_label)
    
    title = "Test Accuracy Comparison"
    plt.title(title)
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy (%)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    _save_plot(title)
    plt.show()

def plot_detailed_loss(path: str, window: int = 50, title: str = None):
    """Plots the detailed STEP-LEVEL loss."""
    data = _load_log(path)
    if not data: return

    raw_loss = data['training_results'].get('detailed_loss', [])
    if not raw_loss:
        print("[Warn] No detailed loss data found.")
        return

    smooth_loss = _smooth(raw_loss, window)
    l = _get_label(data)

    plt.figure(figsize=(12, 4))
    plt.plot(raw_loss, color='lightgray', alpha=0.5, label='Raw Step Loss')
    plt.plot(smooth_loss, linewidth=1.5, color='#1f77b4', label=f'Smoothed (MA={window}) - {l[0]} ({l[1]})')
    
    final_title = title if title else f"Training Loss over Steps: {l[0]}"
    plt.title(final_title)
    plt.xlabel("Step")
    plt.ylabel("Loss")
    plt.legend()
    
    _save_plot(final_title)
    plt.show()

def plot_gradient_norm(path: str, window: int = 50, title: str = None):
    """Plots the L2 NORM of gradients over time."""
    data = _load_log(path)
    if not data: return

    grad_norms = data['training_results'].get('detailed_grad_norm', [])
    if not grad_norms:
        print("[Warn] No gradient norm data found.")
        return
    l = _get_label(data)

    smooth_grad = _smooth(grad_norms, window)

    plt.figure(figsize=(12, 4))
    plt.plot(grad_norms, color='lightgray', alpha=0.5, label='Raw Grad Norm')
    plt.plot(smooth_grad, linewidth=1.5, label=f'Smoothed (MA window={window}) - {l[0]}')
    
    final_title = title if title else "Gradients Norm"
    plt.title(final_title)
    plt.xlabel("Step")
    plt.ylabel("Gradient Norm")
    plt.legend()
    
    _save_plot(final_title)
    plt.show()

def plot_confusion_matrix(path: str):
    """Visualizes the Confusion Matrix for the final epoch."""
    data = _load_log(path)
    if not data: return

    eval_data = data['training_results'].get('final_evaluation', {})
    if not eval_data:
        print("[Warn] No confusion matrix data found.")
        return

    y_true = eval_data['y_true']
    y_pred = eval_data['y_pred']
    
    cm = np.zeros((10, 10), dtype=int)
    for t, p in zip(y_true, y_pred):
        cm[t][p] += 1

    plt.figure(figsize=(8, 7))
    if HAS_SNS:
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
    else:
        plt.imshow(cm, cmap='Blues')
        plt.colorbar()
        for i in range(10):
            for j in range(10):
                plt.text(j, i, cm[i, j], ha="center", va="center", color="black")

    l = _get_label(data)
    title = f"Confusion Matrix: {l[0]}"
    plt.title(title)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.grid(False)
    
    _save_plot(title)
    plt.show()

def plot_efficiency_frontier(log_paths: List[str]):
    """Plots the 'Efficiency Frontier' (Pareto Frontier) bubble chart."""
    params, accs, times, labels = [], [], [], []

    for path in log_paths:
        data = _load_log(path)
        if not data: continue
        
        stats = data['training_results']['per_epoch_stats']
        cfg = data.get('config', {})
        if not stats: continue
        
        p_count = cfg.get('parameter_count', 0)
        final_acc = stats[-1]['accuracy'] * 100
        total_time = sum(s['time'] for s in stats)
        
        if p_count > 0:
            params.append(p_count)
            accs.append(final_acc)
            times.append(total_time)
            l = _get_label(data)
            labels.append(f"{l[0]}")

    if not params:
        print("[Error] No valid data extracted from logs.")
        return

    plt.figure(figsize=(10, 6))
    scatter = plt.scatter(params, accs, c=times, s=150, cmap='viridis_r', edgecolors='black', alpha=0.8)
    
    cbar = plt.colorbar(scatter)
    cbar.set_label('Total Training Time (s)')
    
    plt.xscale('log')
    plt.xlabel("Number of Parameters (Log Scale)")
    plt.ylabel("Test Accuracy (%)")
    
    title = "Efficiency Frontier (Upper-Left is Better)"
    plt.title(title)
    
    for i, label in enumerate(labels):
        plt.annotate(label, (params[i], accs[i]), xytext=(5, 5), textcoords='offset points', fontsize=9)
        
    plt.grid(True, which="both", ls="--", alpha=0.5)
    
    _save_plot(title)
    plt.show()

def plot_system_benchmark(custom_path: str, torch_path: str):
    """Plots a system benchmark comparison."""
    with open(custom_path, 'r') as f: c_data = json.load(f)
    with open(torch_path, 'r') as f: t_data = json.load(f)
    
    c_stats = c_data['training_results']['per_epoch_stats']
    t_stats = t_data['training_results']['per_epoch_stats']
    
    epochs = len(c_stats)
    c_time = np.mean([s['time'] for s in c_stats])
    t_time = np.mean([s['time'] for s in t_stats])
    
    c_mem = np.max([s['memory_peak'] for s in c_stats])
    t_mem = np.max([s['memory_peak'] for s in t_stats])
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    labels = ['Custom Framework', 'PyTorch']
    colors = ['#4c72b0', '#55a868']
    width = 0.5
    
    # Subplot 1: Training Time
    times = [c_time, t_time]
    bars1 = ax1.bar(labels, times, width, color=colors, alpha=0.8)
    ax1.set_ylabel('Avg Time per Epoch (seconds)')
    ax1.set_title(f'Training Speed Comparison')
    ax1.grid(axis='y', linestyle='--', alpha=0.5)
    
    for bar in bars1:
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.2f} s', ha='center', va='bottom', fontsize=12, fontweight='bold')

    # Subplot 2: Memory Usage
    mems = [c_mem, t_mem]
    bars2 = ax2.bar(labels, mems, width, color=colors, alpha=0.8)
    ax2.set_ylabel('Peak Memory Usage (MB)')
    ax2.set_title(f'Memory Efficiency Comparison')
    ax2.grid(axis='y', linestyle='--', alpha=0.5)
    
    for bar in bars2:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f} MB', ha='center', va='bottom', fontsize=12, fontweight='bold')
    
    title = "Benchmark Comparison"
    plt.suptitle(title)
    plt.tight_layout()
    
    _save_plot(title)
    plt.show()

def plot_multi_loss(log_paths: List[str], title: str = "Multi-Run Loss Comparison"):
    """Plots Training Loss curves for an arbitrary number of logs."""
    plt.figure(figsize=(10, 6))
    for path in log_paths:
        data = _load_log(path)
        if not data: continue
        
        stats = data['training_results']['per_epoch_stats']
        epochs = [s['epoch'] for s in stats]
        losses = [s['loss'] for s in stats]

        l = _get_label(data)
        plt.plot(epochs, losses, marker='o', linewidth=2, label=f"{l[0]} ({l[1]})")
    
    plt.title(title)
    plt.xlabel("Epoch")
    plt.ylabel("Training Loss")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    _save_plot(title)
    plt.show()

def plot_multi_accuracy(log_paths: List[str], title: str = "Multi-Run Accuracy Comparison"):
    """Plots Test Accuracy curves for an arbitrary number of logs."""
    plt.figure(figsize=(10, 6))
    for path in log_paths:
        data = _load_log(path)
        if not data: continue
        
        stats = data['training_results']['per_epoch_stats']
        epochs = [s['epoch'] for s in stats]
        accs = [s['accuracy'] * 100 for s in stats]
        
        l = _get_label(data)
        plt.plot(epochs, accs, marker='s', linewidth=2, label=f"{l[0]} ({l[1]})")
    
    plt.title(title)
    plt.xlabel("Epoch")
    plt.ylabel("Test Accuracy (%)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    _save_plot(title)
    plt.show()

def plot_multi_grad_norm(log_paths: List[str], window: int = 50, title: str = "Gradient Norm Stability Comparison"):
    """Plots smoothed Gradient Norms for multiple runs."""
    plt.figure(figsize=(12, 5))
    for path in log_paths:
        data = _load_log(path)
        if not data: continue
        
        grad_norms = data['training_results'].get('detailed_grad_norm', [])
        if not grad_norms: continue
        
        smooth_grad = _smooth(grad_norms, window)
        l = _get_label(data)
        plt.plot(smooth_grad, linewidth=1.5, label=f"{l[0]} ({l[1]})")
    
    plt.title(title)
    plt.xlabel(f"Step (Smoothed window={window})")
    plt.ylabel("Gradient L2 Norm")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    _save_plot(title)
    plt.show()

def plot_performance_bar(log_paths: List[str], title: str = "Performance Benchmark: Time & Memory"):
    """Bar chart comparing Total Training Time and Peak Memory."""
    def _short_framework(model_label: str) -> str:
        if "PyTorch" in model_label: return "PyTorch"
        if "Custom" in model_label: return "Custom"
        parts = (model_label or "").split()
        return parts[0] if parts else "Unknown"

    labels, times, mems = [], [], []

    for path in log_paths:
        data = _load_log(path)
        if not data: continue
        stats = data.get('training_results', {}).get('per_epoch_stats', [])
        if not stats: continue
        model_label, opt_label = _get_label(data)
        labels.append(f"{_short_framework(model_label)} ({opt_label})")
        times.append(sum(s.get('time', 0.0) for s in stats))
        mems.append(max(s.get('memory_peak', 0.0) for s in stats))

    x = np.arange(len(labels))
    width = 0.35
    time_color = "#4c72b0"
    mem_color  = "#55a868"

    fig, ax1 = plt.subplots(figsize=(12, 6))

    rects1 = ax1.bar(x - width/2, times, width, label='Total Time (s)', color=time_color, alpha=0.85)
    ax1.set_ylabel('Time (s)', color=time_color, fontweight='bold')
    ax1.tick_params(axis='y', labelcolor=time_color)
    ax1.set_title(title)
    ax1.set_xticks(x)
    ax1.set_xticklabels(labels, rotation=0, ha="center")

    ax2 = ax1.twinx()
    rects2 = ax2.bar(x + width/2, mems, width, label='Peak Memory (MB)', color=mem_color, alpha=0.85)
    ax2.set_ylabel('Memory (MB)', color=mem_color, fontweight='bold')
    ax2.tick_params(axis='y', labelcolor=mem_color)

    ax1.legend([rects1, rects2], ['Total Time (s)', 'Peak Memory (MB)'], loc='upper right')
    fig.tight_layout()
    ax1.grid(False)
    ax2.grid(False)
    
    _save_plot(title)
    plt.show()